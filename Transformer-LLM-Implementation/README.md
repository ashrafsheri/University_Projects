# Transformer Language Model Implementation (smolLM)

A from-scratch implementation of a transformer-based language model following the LLaMA architecture, featuring grouped-query attention, rotary positional embeddings, and advanced architectural components.

## ğŸ¯ Project Overview

This project implements a compact language model (smolLM) with 135M parameters, featuring:
- **Grouped-Query Attention (GQA)** for efficient multi-head attention
- **Rotary Position Embeddings (RoPE)** for better sequence modeling
- **RMS Normalization** for stable training
- **Gated MLP layers** with SiLU activation
- Complete decoder stack with residual connections

## ğŸ—ï¸ Architecture

### Model Components

1. **Grouped-Query Attention**
   - 9 query heads, 3 key-value heads
   - Head dimension: 64
   - Implements efficient attention with reduced memory footprint

2. **Rotary Embeddings**
   - Base frequency: 10,000
   - Applied to queries and keys for position-aware attention

3. **Decoder Blocks**
   - 30 stacked decoder layers
   - Pre-normalization with RMSNorm
   - Gated FFN with intermediate size 1536

4. **Configuration**
   ```python
   vocab_size = 49,152
   hidden_size = 576
   intermediate_size = 1,536
   num_hidden_layers = 30
   num_heads = 9
   kv_heads = 3
   ```

## ğŸ“ Project Structure

```
Transformer-LLM-Implementation/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ config.py                 # Model configuration dataclass
â”œâ”€â”€ attention.py              # Grouped-query attention implementation
â”œâ”€â”€ layers.py                 # Decoder blocks and MLP layers
â”œâ”€â”€ model.py                  # Main smolLM model classes
â”œâ”€â”€ generate.py               # Text generation utilities
â”œâ”€â”€ test_model.py             # Model validation and testing
â””â”€â”€ __pycache__/             # Python cache files
```

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install torch numpy transformers
```

### Quick Start

```python
from config import smolConfig
from model import smolLM
import torch

# Initialize model
config = smolConfig()
model = smolLM(config)

# Example forward pass
input_ids = torch.randint(0, config.vocab_size, (1, 128))
attention_mask = torch.ones_like(input_ids)

outputs = model(input_ids, attention_mask)
logits = outputs['logits']  # Shape: (batch_size, seq_len, vocab_size)
```

### Text Generation

```python
from generate import generate_text
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-135M")
prompt = "The future of AI is"

generated = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_length=50
)
print(generated)
```

## ğŸ”§ Implementation Details

### Grouped-Query Attention

GQA reduces the number of key-value heads while maintaining multiple query heads, providing a balance between:
- Multi-Query Attention (MQA): 1 KV head
- Multi-Head Attention (MHA): Equal query and KV heads

Benefits:
- Reduced memory usage during inference
- Faster decoding while maintaining model quality
- Better cache efficiency for auto-regressive generation

### Rotary Position Embeddings

RoPE encodes absolute positions with rotation matrices and naturally includes relative position information:

```python
# Frequency computation
freq = 1.0 / (base ** (torch.arange(0, dim, 2) / dim))

# Apply rotation to Q and K
q_embed = (q * cos) + (rotate_half(q) * sin)
k_embed = (k * cos) + (rotate_half(k) * sin)
```

### Weight Tying

The embedding layer and LM head share weights, reducing parameters and often improving performance:

```python
self.lm_head.weight = self.model.embed_tokens.weight
```

## ğŸ“Š Model Testing

Run the test suite to validate the implementation:

```bash
python test_model.py
```

Tests include:
- Forward pass correctness
- Output shape validation
- Gradient flow verification
- Attention mask application
- Generation functionality

## ğŸ“ Learning Outcomes

This implementation demonstrates:
- âœ… Modern transformer architecture patterns
- âœ… Efficient attention mechanisms (GQA)
- âœ… Advanced positional encoding (RoPE)
- âœ… Normalization techniques (RMSNorm)
- âœ… Residual connections and skip paths
- âœ… Auto-regressive language modeling

## ğŸ“š References

- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [SmolLM by HuggingFace](https://huggingface.co/HuggingFaceTB/SmolLM-135M)

## ğŸ” Key Features

- **Educational**: Clear, well-documented code for learning transformer internals
- **Modular**: Separated components for easy understanding and modification
- **Complete**: Full implementation from embeddings to generation
- **Tested**: Comprehensive test suite for validation

## ğŸ“ Notes

- Model follows LLaMA architecture conventions
- Causal attention mask ensures auto-regressive generation
- RMS normalization provides training stability
- Gated MLP improves model expressiveness

## ğŸ™ Acknowledgments

Implementation based on the LLaMA and SmolLM architectures, with inspiration from the transformer literature and HuggingFace implementations.
