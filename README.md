# Deep Learning Portfolio

A collection of deep learning projects showcasing implementations of modern neural network architectures and training techniques.

## ğŸš€ Projects

### 1. [Transformer LLM Implementation](./Transformer-LLM-Implementation)
**A from-scratch transformer language model following the LLaMA architecture**

- ğŸ—ï¸ **Architecture**: 135M parameter model with grouped-query attention
- ğŸ”‘ **Key Features**: Rotary embeddings, RMSNorm, gated MLPs
- ğŸ“š **Concepts**: Transformer blocks, attention mechanisms, autoregressive generation
- ğŸ› ï¸ **Tech Stack**: PyTorch, transformers

**Highlights**: Complete implementation of modern transformer components including grouped-query attention for efficient inference and rotary position embeddings for better sequence modeling.

---

### 2. [PixelCNN Generative Model](./PixelCNN-Generative-Model)
**Autoregressive image generation using masked convolutions**

- ğŸ¨ **Task**: Generate MNIST digits pixel-by-pixel
- ğŸ”‘ **Key Features**: Masked convolutions (Type A/B), residual blocks
- ğŸ“š **Concepts**: Autoregressive modeling, causal constraints, sequential generation
- ğŸ› ï¸ **Tech Stack**: PyTorch, torchvision

**Highlights**: Implements causal masking to ensure each pixel is generated conditionally on all previous pixels, demonstrating how CNNs can model sequential dependencies in images.

---

### 3. [DPO LLM Alignment](./DPO-LLM-Alignment)
**Direct Preference Optimization for aligning language models with human preferences**

- ğŸ¯ **Task**: Align smolLM with human preferences without RL
- ğŸ”‘ **Key Features**: Contrastive preference learning, frozen reference model
- ğŸ“š **Concepts**: RLHF without rewards, preference optimization, alignment
- ğŸ› ï¸ **Tech Stack**: PyTorch, transformers, datasets

**Highlights**: Demonstrates modern alignment techniques that bypass complex reinforcement learning, using direct optimization on preference pairs for simpler and more stable training.

---

### 4. [Diffusion Models](./Diffusion-Models-MNIST)
**From DDPM to CLIP-guided text-to-image generation**

- ğŸŒ«ï¸ **Tasks**: 
  - Unconditional DDPM
  - Class-conditional generation
  - CLIP-guided text-to-image
- ğŸ”‘ **Key Features**: Noise scheduling, U-Net with attention, CLIP guidance
- ğŸ“š **Concepts**: Denoising diffusion, reverse processes, guidance
- ğŸ› ï¸ **Tech Stack**: PyTorch, CLIP, transformers

**Highlights**: Progressive implementation from basic diffusion to text-guided generation, showcasing the power of iterative denoising and multi-modal conditioning.

---

### 5. [GNN Maze Solver](./GNN-Maze-Solver)
**Learning navigation policies using Graph Neural Networks**

- ğŸ—ºï¸ **Task**: Navigate mazes using learned GNN policies
- ğŸ”‘ **Key Features**: GraphSAGE, ego-graphs, imitation learning
- ğŸ“š **Concepts**: Graph representation learning, spatial reasoning, policy networks
- ğŸ› ï¸ **Tech Stack**: PyTorch Geometric, NetworkX

**Highlights**: Demonstrates how GNNs can learn effective navigation from local graph views, achieving near-optimal performance while using only partial information.

---

## ğŸ“ Technical Skills Demonstrated

### Deep Learning Fundamentals
- âœ… Neural network architecture design
- âœ… Loss function engineering
- âœ… Optimization strategies
- âœ… Regularization techniques
- âœ… Training dynamics analysis

### Advanced Architectures
- ğŸ”· **Transformers**: Self-attention, positional encodings, decoder stacks
- ğŸ”· **CNNs**: Masked convolutions, residual connections, U-Nets
- ğŸ”· **GNNs**: Message passing, neighborhood aggregation, graph pooling
- ğŸ”· **Diffusion Models**: Noise schedules, denoising networks, guidance

### Training Techniques
- ğŸ“Š Supervised learning from optimal demonstrations
- ğŸ“Š Preference-based optimization
- ğŸ“Š Multi-modal conditioning (text, class labels)
- ğŸ“Š Imitation learning from expert policies
- ğŸ“Š Efficient fine-tuning strategies

### Implementation Skills
- ğŸ’» PyTorch ecosystem (torch, torchvision, torch-geometric)
- ğŸ’» Model evaluation and visualization
- ğŸ’» Data preprocessing pipelines
- ğŸ’» Custom layer implementations
- ğŸ’» Gradient computation and backpropagation

## ğŸ“Š Project Complexity Matrix

| Project | Architecture Complexity | Training Complexity | Novelty | Lines of Code |
|---------|------------------------|---------------------|---------|---------------|
| Transformer LLM | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | ~500 |
| PixelCNN | â­â­â­â­ | â­â­â­ | â­â­â­â­ | ~400 |
| DPO Alignment | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | ~300 |
| Diffusion Models | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | ~600 |
| GNN Maze Solver | â­â­â­ | â­â­â­ | â­â­â­ | ~350 |

## ğŸ› ï¸ Technology Stack

### Core Frameworks
- **PyTorch**: Deep learning framework
- **PyTorch Geometric**: Graph neural networks
- **Transformers**: Pre-trained models and tokenizers
- **CLIP**: Vision-language models

### Supporting Libraries
- **NumPy**: Numerical computing
- **NetworkX**: Graph operations
- **Matplotlib**: Visualization
- **TQDM**: Progress bars
- **Datasets**: HuggingFace datasets

### Development Tools
- **Jupyter**: Interactive notebooks
- **Git**: Version control
- **Python 3.8+**: Programming language

## ğŸ“ˆ Learning Progression

The projects are ordered by conceptual complexity and build upon each other:

1. **Start**: Transformer basics (attention, embeddings)
2. **Progress**: Autoregressive models (PixelCNN)
3. **Advance**: Alignment techniques (DPO)
4. **Expand**: Generative models (Diffusion)
5. **Apply**: Graph learning (GNN)

## ğŸ¯ Key Achievements

- âœ… Implemented 5 major deep learning architectures from scratch
- âœ… Trained models on diverse tasks (language, images, graphs)
- âœ… Applied modern techniques (attention, diffusion, preference learning)
- âœ… Achieved competitive performance vs. baselines
- âœ… Comprehensive documentation and visualization

## ğŸ“š References & Inspiration

Each project includes detailed references to:
- Original research papers
- Implementation guides
- Theoretical background
- Best practices

## ğŸš€ Getting Started

Each project folder contains:
- ğŸ“„ **README.md**: Detailed project documentation
- ğŸ““ **Notebooks**: Jupyter notebooks with implementations
- ğŸ **Scripts**: Python files for reusability
- ğŸ“Š **Results**: Training curves and generated samples

### Prerequisites

```bash
# Create virtual environment
python -m venv dl_env
source dl_env/bin/activate  # On Windows: dl_env\Scripts\activate

# Install dependencies (per project)
pip install torch torchvision
pip install transformers datasets
pip install torch-geometric
pip install networkx matplotlib tqdm
```

### Quick Navigation

```bash
# Clone and explore
cd Transformer-LLM-Implementation  # Start with transformers
cd ../PixelCNN-Generative-Model    # Explore autoregressive models
cd ../DPO-LLM-Alignment            # Learn alignment techniques
cd ../Diffusion-Models-MNIST       # Master diffusion models
cd ../GNN-Maze-Solver              # Apply graph learning
```

## ğŸ“ Project Structure

Each project follows a consistent structure:

```
Project-Name/
â”œâ”€â”€ README.md                 # Comprehensive documentation
â”œâ”€â”€ notebook.ipynb           # Implementation notebook
â”œâ”€â”€ script.py                # Python script version
â””â”€â”€ (additional files)       # Model weights, utilities, etc.
```

## ğŸ“ Educational Value

These projects demonstrate:

1. **Theoretical Understanding**: Deep knowledge of architectures and algorithms
2. **Practical Implementation**: Ability to translate papers to code
3. **Debugging Skills**: Resolving training issues and convergence problems
4. **Optimization**: Efficient implementation and hyperparameter tuning
5. **Documentation**: Clear explanations and reproducible results

## ğŸŒŸ Highlights by Category

### ğŸ—ï¸ Architecture Design
- Custom transformer blocks with modern components
- U-Net with attention for diffusion models
- GraphSAGE for graph representation learning
- Masked convolutions for causal modeling

### ğŸ¯ Training Innovations
- Preference-based optimization (DPO)
- CLIP-guided generation
- Imitation learning from optimal policies
- Multi-task conditioning (class, text)

### ğŸ“Š Evaluation & Analysis
- Quantitative metrics (loss, accuracy, path length)
- Qualitative assessment (generated images, text)
- Comparative analysis (GNN vs. Dijkstra)
- Ablation studies (schedule, architecture choices)

## ğŸ”® Future Directions

Potential extensions:
- ğŸ”„ Combine techniques (e.g., GNN + Diffusion for graph generation)
- ğŸ“ˆ Scale to larger models and datasets
- ğŸ® Apply to real-world problems (robotics, NLP, computer vision)
- ğŸ”¬ Experiment with latest research (Mamba, Hyena, JEPA)

## ğŸ“¬ Contact & Collaboration

These projects represent a comprehensive exploration of modern deep learning techniques, from foundational architectures to cutting-edge alignment methods.

---

**Portfolio Statistics**:
- ğŸ“ 5 Major Projects
- ğŸ“ ~2,150 Lines of Code
- ğŸ§  10+ Neural Network Architectures
- ğŸ“š 50+ Research Papers Referenced
- â±ï¸ 100+ Hours of Implementation

**Last Updated**: November 2025
